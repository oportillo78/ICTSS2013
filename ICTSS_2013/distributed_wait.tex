
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Author: Omar Portillo
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%Abstract: 0.5
%Intro: 2
%Bg: 2
%Approach/Architecture: 3
%Experiments/Results: 5
%Related Work: 1
%Conclusions: 0.5
%References:  1

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Defining document class + packages to be used
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[runningheads,a4paper]{llncs}

\usepackage{amssymb}
\setcounter{tocdepth}{3}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}

\usepackage{url}
\newcommand{\keywords}[1]{\par\addvspace\baselineskip
\noindent\keywordname\enspace\ignorespaces#1}

%Commands needed to define the max size a float can have without been presented
% ALONE in a page.
\renewcommand\topfraction{0.85}
\renewcommand\bottomfraction{0.85}
\renewcommand\textfraction{0.1}
\renewcommand\floatpagefraction{0.85}

%Commands to redice the space before and after floats (like table,fig or alg)
\setlength\floatsep{1.25\baselineskip plus 3pt minus 2pt}
\setlength\textfloatsep{1.25\baselineskip plus 3pt minus 2pt}
\setlength\intextsep{1.25\baselineskip plus 3pt minus 2 pt}

\setlength{\belowcaptionskip}{-7.5pt}
\setlength{\abovecaptionskip}{-2.5pt}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% TITLE + AUTHORS' INFORMATION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\mainmatter  % start of an individual contribution

% first the title is needed
\title{Towards an Automated Approach to Use Expert Systems
in Java Performance Testing}

% a short form should be given in case it is too long for the running head
\titlerunning{Automated Approach to Use Expert Systems
in Performance Testing}

% the name(s) of the author(s) follow(s) next
%
% NB: Chinese authors should write their first names(s) in front of
% their surnames. This ensures that the names appear correctly in
% the running heads and the author index.
%
\author{A. Omar Portillo-Dominguez\inst{1}\and Miao Wang\inst{1}\and Philip
Perry\inst{1} \and\\
John Murphy\inst{1} \and Nick Mitchell\inst{2}\and Peter F. Sweeney\inst{2}\and
Erik Altman\inst{2}}

\authorrunning{A.O. Portillo-Dominguez et al.}
% (feature abused for this document to repeat the title also on left hand pages)

\urldef{\mailucdconnect}\path|andres.portillo-dominguez@ucdconnect.ie,|
\urldef{\mailucd}\path|{philip.perry,miao.wang,j.murphy}@ucd.ie,|
\urldef{\mailibm}\path|{nickm,pfs,ealtman}@us.ibm.com|

% the affiliations are given next; don't give your e-mail address
% unless you accept that it will be published
\institute{Lero - The Irish Software Engineering Research Centre, Performance\\
Engineering Laboratory, UCD School of Computer Science and Informatics,
University College Dublin, Ireland\\
\mailucdconnect\\
\mailucd\\
%\url{http://www.ucd.ie/}
\and
IBM T.J. Watson Research Center,\\
Yorktown Heights, New York, USA\\
\mailibm}%\\
%\url{http://www.ibm.com/}

\toctitle{Lecture Notes in Computer Science}
\tocauthor{Authors' Instructions}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Abstract 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-15pt}
\begin{abstract}
Performance testing in highly distributed environments is a very challenging
task. Specifically, the identification of performance issues and
the diagnosis of their root causes are time-consuming and complex
activities which usually require multiple tools and heavily rely on expertise. 
To simplify the above tasks, hence increasing the productivity and reducing the
dependency on expert knowledge, many researchers have been developing tools with 
built-in expertise knowledge for non-expert users. However, managing the huge
volume of data generated by these tools in highly distributed
environments prevent their efficient usage in performance testing. To
address these limitations, this paper presents a lightweight approach to automate the usage of 
expert tools in performance testing. Moreover, we use a tool named Whole-system 
Analysis of Idle Time to demonstrate how our research work solves this problem. The 
validation involved two case studies using real-life applications, which assessed the
overhead of the proposed approach and the time savings that it can bring to the
analysis of performance issues. The results proved the benefits of the approach by achieving
a significant decrease in the time invested in performance analysis while 
introducing a low overhead in the tested system.

\vspace{-3pt}
\keywords{Performance testing, automation, performance analysis, idle-time
analysis, distributed environments, multi-tier applications}
\end{abstract}
\vspace{-22pt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Introduction
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-4pt}
\section{Introduction}
\vspace{-7pt}
It is an accepted fact in the industry that performance is a critical dimension
of quality and should be a major concern of any software project. This is
especially true at enterprise-level, where system performance plays a central
role in using the software system to achieve business goals. However it is not
uncommon that performance issues occur and materialize into serious problems in a significant percentage of applications
(i.e. outages on production environments or even cancellation of
software projects). For example, a 2007 survey applied to information technology
executives \cite{Compuware1} reported that 50\% of them had faced performance 
problems in at least 20\% of their deployed applications.

This situation is partially explained by the pervasive nature of
performance, which makes it hard to assess because performance is practically
influenced by every aspect of the design, code, and execution environment
of an application. Latest trends in the information technology world (such as
Service Oriented
Architecture\footnote{http://msdn.microsoft.com/en-us/library/aa480021.aspx} and 
Cloud Computing\footnote{http://csrc.nist.gov/publications/nistpubs/800-145/SP800-145.pdf}) 
have also augmented the complexity of applications further complicating activities related to
performance. 

Under these conditions, it is not surprising that doing performance
testing is complex and time-consuming. A special challenge, documented by
multiple authors \cite{Woodside2007,trevor1,Angelopoulos2012}, is that current
performance tools heavily rely on expert knowledge to understand their output.
Also multiple sources are commonly required to diagnose performance problems,
especially in highly distributed environments. For instance in Java: thread dumps, 
garbage collection logs, heap dumps, CPU utilization and memory usage, are a few
examples of the information that a tester could need to understand the performance 
of an application. This problem increases the expertise required to do
performance analysis, which is usually held by only a small number of experts 
inside an organization\cite{Spear2009}. Therefore it could potentially lead to
bottlenecks where certain activities can only be done by these experts,
impacting the productivity of the testing teams\cite{Angelopoulos2012}.

To simplify the performance analysis and diagnosis tasks, hence increasing the
productivity and reducing the dependency on expert knowledge, many researchers
have been developing tools with built-in expertise knowledge for non-expert
users \cite{Altman2010,pat7,Angelopoulos2012}. However, various limitations
exist in these tools that prevent their usage in the performance testing of
highly distributed environments. For example, the data collection usually needs
to be controlled manually which, in an environment composed of multiple nodes to
monitor and coordinate simultaneously, is very time-consuming and error-prone
because this process would produce a vast amount of data to be collected and
consolidated. This challenge is more complex when the data also needs to be
processed periodically during a test execution to get an incremental view of the 
results. A similar situation occurs with the outputs of an expert system, where
a tester commonly needs to review multiple reports (possible one for each monitored 
node per data processing cycle). This situation reduces the usefulness of the
reports, as a tester must manually correlate their findings.

Even though these limitations might be manageable in small testing environments,
they prevent the usage of these tools in bigger environments. To
exemplify this problem, let's use the Eclipse Memory Analyzer
Tool\footnote{http://www.eclipse.org/mat/} (MAT), which is a popular open source
tool to identify memory consumption issues in Java. If a tester wants to use MAT
to monitor an environment composed of 100 nodes during a 24-hour test run and 
get incremental results every hour, she would need to manually coordinate the
data gathering of memory snapshots and the generation of the tool's reports.
These steps conducted periodically for all the nodes every hour, which
produces a total of 2400 iterations. Moreover the tester would have to review
the multiple reports she would get every hour to evaluate if any memory issues
exist. As an alternative, she may concentrate the analysis on a single node,
assuming it is representative of the whole system. However it generates the risk of
potentially overlooking issues in the tested application.

In addition to the previous challenges, the overhead generated by any technique
should be low and steady through time to minimize the impact it has in the
tested environment (i.e. inaccurate results or abnormal behavior), otherwise the technique would
not be suitable for performance testing. For example,
instrumentation\footnote{http://msdn.microsoft.com/en-us/library/aa983649(VS.71).aspx}
is currently a common approach used in performance analysis to gather input data
\cite{Yang1,Hangal1,Csallner1,Chen2}. However, it has the downside of obscuring
the performance of the instrumented applications, hence compromising the results of 
performance testing. Moreover it usually requires the modification of the source
code, which might not always be possible such as in third-party components.
Similarly, if a tool requires heavy human effort to be used, this might limit the 
applicability of that tool. On the contrary, automation could play a key role to encourage 
the adoption of a technique. As documented by the authors in \cite{Shahamiri1},
this strategy has proven successful in performance testing activities.

Finally, to ensure that our research work is helpful for solving real-life
problems in the software industry, we have been working with our industrial
partner, IBM System Verification Test (SVT), to understand the challenges
that they experience in their day-to-day testing activities. The received
feedback confirms that there is a real need to simplify the usage of expert
tools so that testers can carry out analysis tasks in less time.

This paper proposes a lightweight automation approach that addresses the common
usage limitations of an expert system in performance testing. Furthermore,
during our research development work we have successfully applied our approach
to the IBM Whole-system Analysis of Idle Time tool (WAIT)\footnote{http://wait.ibm.com}. 
This publicly available tool is a lightweight expert system that helps to
identify the main performance inhibitors that exist in Java systems. Our work was 
validated through two case studies using real-world applications. The first case study 
concentrated in evaluating the overhead introduced by our approach. The second
assessed the productivity gains that the approach can bring to the
performance testing process. The results provided evidence about
the benefits of this approach: It drastically reduced the effort required by a
tester to use and analyze the outputs of the selected expert tool (WAIT). This
usage simplification translated into a quicker identification of performance issues, 
including the pinpointing of the responsible classes and methods. Also the
introduced overhead was low (between 0\% to 3\% when using a common industry
\emph{Sampling Interval}).

The main contributions of this paper are: 
%\\\\

1. A novel lightweight approach to automate the usage of expert systems in
performance testing of distributed software systems.

2. A practical validation of the approach consisting of an implementation
around the WAIT tool and two case studies using real-life applications.

3. Analysis of the overhead the approach has in the monitored environment.
\\\\
The rest of this paper is structured as follows: Section \ref{Background}
discusses the background. Section \ref{ProposedApproach} explains the proposed
approach, while Section \ref{ExperimentalEvaluation} explores the experimental 
evaluation and results. Section \ref{RelatedWork} shows the related work.
Finally Section \ref{Conclusions} presents the conclusions and future work.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Background
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-7pt}
\section{Background}
\label{Background}
\vspace{-7pt}
\emph{Idle-time analysis} is a methodology that is used to identify the root
causes that lead to under-utilized resources. This approach, proposed in
\cite{Altman2010}, is based on the observed behavior that performance problems in multi-tier
applications usually manifest as idle time indicating a lack of motion.
WAIT is an expert system that implements the idle-time analysis and identifies
the main performance inhibitors that exist on a system. Moreover it has proven successful in simplifying the detection of performance issues and their root causes in Java
systems \cite{Altman2010,Wu1}.

WAIT is based on non-intrusive sampling mechanisms available at
Operating System level (i.e. ``ps'' command in a Unix environment) and the Java
Virtual Machine (JVM), in the form of \emph{Javacores}
\footnote{http://www-01.ibm.com/support/docview.wss?uid=swg27017906\&aid=1}
(diagnostic feature to get a quick snapshot of the JVM state, offering
information such as threads, locks and memory). The fact that WAIT uses standard
data sources makes it non-disruptive, as no special flags or instrumentation are
required to use it. Furthermore WAIT requires infrequent samples to perform its
diagnosis, so it also has low overhead.

From an end-user perspective, WAIT is simple: A user only needs to
collect as much data as desired, upload it to a public web page and obtain a
report with the findings. This process can be repeated multiple times to monitor a
system through time. Internally, WAIT uses an engine built on top of a set of 
expert rules that perform the analysis. \figurename ~\ref{fig_WAITReport} shows an
example of a WAIT Report. The top area summarizes the usage of resources (i.e.
CPU or memory) and the number and type of threads. The bottom section shows all
the performance inhibitors that have been identified, ranked by frequency and impact. 
Also each color indicates a different category of problem. For example, in
\figurename ~\ref{fig_WAITReport} the top issue appeared in 53\% of the samples
and affected 7.6 threads on average. Moreover the affected resource was the
network (yellow color) and was caused by database readings.

\begin{figure}[!h]
\centering
\includegraphics[totalheight=.3\textheight,width=1.0\textwidth]{bigExampleReportConsolidated}
\caption{Example of WAIT Report}
\label{fig_WAITReport}
\end{figure}

Given its strengths, WAIT is a promising candidate to reduce the
dependence on a human expert and reduce the time required for performance 
analysis. However, as with many expert systems that could be used for testing 
distributed software systems, the volume of data generated can be difficult to 
manage and efficiently process this data can be an impediment to their
adoption. The effort required to manually collect data to feed WAIT and the number 
of reports a tester gets from the WAIT system are approximately linear with
respect to the number of nodes and the update frequency of the results. This makes 
WAIT a good candidate to apply our proposed approach.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Proposed Approach
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-7pt}
\section{Proposed Approach and Architecture}
\label{ProposedApproach}
\vspace{-7pt}

\subsection{Proposed Approach}
\vspace{-7pt}
The objective of this work was to automate the manual processes involved
in the usage of an expert system (ES). This logic will execute concurrently with
the performance test, periodically collecting the required samples, then
incrementally processing them with the expert system to get a consolidated
output. This scenario is depicted in \figurename ~\ref{fig_Overview} where the
automated logic shields the tester from the complexities of using the ES, so
that she only needs to interact with her load testing tool during the whole duration 
of a performance test.

\begin{figure}[!h]
\centering
\includegraphics[totalheight=.20\textheight,width=0.8\textwidth]{architecture_dwait}
\caption{Contextual View of the Proposed Approach}
\label{fig_Overview}
\end{figure}

Moreover the detailed approach is depicted in the \figurename
~\ref{fig_ApproachDiagram}. It will start right after the performance
test has started, requiring some additional inputs: 
\vspace{-7pt}
\begin{itemize}
	\item The list of nodes to be monitored.
	\item The \emph{Sampling Interval} to control how often the samples will be
	collected.
	\item The \emph{Backup} flag to indicate if the collected data should be backed
	up before any cleaning occurs.
	\item The \emph{Time Threshold} to define the maximum time between data
	uploads.
	\item The \emph{Hard Disk Threshold} to define the maximum storage quota for
	collected data (to prevent its uncontrolled growth).
\end{itemize}

\begin{figure}[!h]
\centering
\includegraphics[width=1.0\textwidth]{ApproachDiagram}
\caption{Process Flow - Automation approach}
\label{fig_ApproachDiagram}
\end{figure}

The process starts, in parallel to the performance test, by initializing the
configured parameters. Then it gets a new \emph{RunId}, value which will
uniquely identify this test run and all its collected data and reports. This
value is then propagated to all the nodes. On each node, the \emph{Hard Disk
Usage Threshold} and  the \emph{Next Time Threshold} are initialized. These thresholds 
make the process adaptable to different usages, by allowing a tester to control
the pace of data collection and data feeding to an ES. For example, one tester might 
prefer to get updated results as soon as new data is available. In this case,
she could define a very low \emph{Hard Disk Usage Threshold} or a very small
\emph{Next Time Threshold}. Moreover it is not uncommon that an ES require some 
processing time to generate its outputs. In this scenario, bigger thresholds
would be preferable.

Then each node starts the following loop in parallel until the performance test
finishes: A new set of data samples is collected. After the collection finishes,
the system checks if any of the two thresholds have been reached. If either of
these conditions has occurred, the data is sent to the expert system (labeling the 
data with the \emph{RunId} so that information from different nodes can be
identified as part of the same test run). If a \emph{Backup} was enabled, the data 
is copied to the backup destination before it is deleted to free space and keep
the HD usage below the threshold. As some data collection processes could be
very time-consuming (i.e. the generation of a memory dump in Java can take
several minutes and take hundreds of megabytes of HD), this step could be very
useful as it might allow further off-line analysis of the collected data
without affecting the actual test run. Then updated outputs
from the expert system are retrieved and the \emph{Next Time Threshold} is calculated. 
Finally, the logic awaits the \emph{Sampling Interval} before a new iteration
starts.

Once the performance test finishes, any remaining collected data is sent (and
backed up if configured) so that this information is also processed by the
expert system. Lastly this data is also cleared and the final consolidated
outputs of the expert system are obtained.

\vspace{-7pt}
\subsection{Architecture}
\vspace{-7pt}
The approach is implemented with the architecture
presented in \figurename ~\ref{fig_components}. It is composed of two main components:
The \emph{Control Agent} is responsible of interacting with the load
testing tool (LTT) to know when the performance test starts and ends as well as
to provide the status of the processes in every node. It is also responsible of
getting the runId and propagate it to all the nodes. The second component is the
\emph{Node Agent} which is responsible for the collection, upload, backup and cleanup steps 
in each application node. On each agent, the logic that controls the
approach, as well as some supporting logic within a set of \emph{Helper} classes
(i.e. the calculation of the thresholds in the \emph{Node Agent}), is
independent of the target ES and the LTT. On the contrary, the logic that interacts with these
tools might need to be extended to support them. In order to minimize the
required code changes, this logic is encapsulated within their respective
packages and all interactions are done via interfaces. This is depicted in
\figurename ~\ref{fig_wrapper} which show the class diagram of the package \emph{ES Wrapper}. 
It contains a main interface (\emph{IExpertSystem}) which exposes all callable
actions as well as an abstract class with the generic logic. Then the hierarchy
is extended to support different ES, possibly needing specific classes per
operating system.

\begin{figure*}
\centering
\begin{minipage}[b]{.69\textwidth}

\centering
\includegraphics[totalheight=.25\textheight,width=1.0\textwidth]{Components}
\caption{Proposed Approach - Component Diagram}
\label{fig_components}

\end{minipage}\qquad
\begin{minipage}[b]{.25\textwidth}

\centering
\includegraphics[totalheight=.25\textheight,width=1.0\textwidth]{Wrapper}
\caption{ES Wrapper package - Class Diagram}
\label{fig_wrapper}

\end{minipage}
\end{figure*}

Additionally these two components communicate through commands, following the
\emph{Command} Design Pattern\footnote{http://www.oodesign.com/command-pattern.html}. The
\emph{Control Agent} invokes the commands (i.e. the start and stop based on the
changes of status in the load testing tool); while the \emph{Node Agent}
implements the logic in charge of executing each concrete command. This logic
includes sending back the result of the performed operation to the \emph{Control Agent}.

An example of the distinct interactions that occur between these
components are depicted in \figurename ~\ref{fig_SeqDiagram}. Once a tester has
started a performance test (step 1), the \emph{Control Agent} propagates the
action to each of the nodes (steps 2 to 4). Then each \emph{Node Agent} performs
its periodic data collection (steps 5 to 9) until any of the thresholds is
satisfied and the data is sent to the ES (steps 10 and 11). These processes
continue iteratively until the test ends. At that moment, the \emph{Control
Agent} propagates the stop action to all \emph{Node Agents} (steps 21,22 and
24). At any time during the test execution, the tester might choose to review
the intermediate results obtained from the expert system (steps 12 to 14) until
getting the final results (steps 25 to 27).

%\vspace{-5pt}
\begin{figure}[!h]
\centering
\includegraphics[width=1.0\textwidth]{SequenceDiagram}
\caption{Sequence diagram of the automated approach}
\label{fig_SeqDiagram}
\end{figure}
%\vspace{-5pt}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Experimental Setup / Experiments
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-7pt}
\section{Experimental Evaluation}
\label{ExperimentalEvaluation}
%\vspace{-5pt}

%In this section the developed prototype and the experimental setup are
%presented. Then the experiments are described and their results discussed.

\vspace{-7pt}
\subsection{Prototype}
\vspace{-7pt}
Based on the proposed approach, a prototype has been developed
in conjunction with our industrial partner IBM. The \emph{Control Agent} was
implemented as an Eclipse Plugin for the Rational Performance Tester (RPT)
\footnote{http://www-03.ibm.com/software/products/us/en/performance}, which is a
load testing tool commonly used in the industry; the \emph{Node Agent} was
implemented as a Java Web Application, and WAIT was the selected expert system
due to its analysis capabilities previously discussed in
Section \ref{Background}.

Once the agents are installed in an environment, WAIT can be configured as
any other resource in RPT as shown in \figurename ~\ref{fig_config}. Similarly,
during a performance test WAIT can be monitored as any other resource in the
\emph{Performance Report} of RPT under the \emph{Resource View} as depicted in
\figurename ~\ref{fig_mon}, which shows some of the metrics that a tester can
check: The number of monitored processes, the number of WAIT data collections
that have been triggered and the collections that have occurred. Finally, the
consolidated WAIT report is also accessible within RPT, so a tester does not
need to leave RPT during the whole performance test. This is shown in
\figurename ~\ref{fig_report}. 
%The benefits of this report will be explained in Section
% \ref{Experiment_2_Results} as part of the experimental results.

%--------------------------------------------------
\vspace{-5pt}
\begin{figure*}
\centering
\begin{minipage}[b]{.50\textwidth}

\centering
\includegraphics[totalheight=.27\textheight,width=1.0\textwidth]{WAIT-config}
\caption{WAIT configuration in RPT}
\label{fig_config}

\end{minipage}\qquad
\begin{minipage}[b]{.44\textwidth}

\centering
\includegraphics[totalheight=.27\textheight,width=1.0\textwidth]{WAIT-monitoring}
\caption{WAIT monitored in RPT}
\label{fig_mon}

\end{minipage}
\end{figure*}
\vspace{-5pt}

%--------------------------------------------------

\begin{figure}[!h]
\centering
\includegraphics[totalheight=.35\textheight,width=1.0\textwidth]{WAIT-report}
\caption{WAIT Report accessible in RPT}
\label{fig_report}
\end{figure}

\vspace{-7pt}
\subsection{Experimental Set-up}
\vspace{-7pt}
Two experiments were performed. The first one aimed to evaluate if the overhead
introduced by the proposed approach was low so that it does not compromise the
results of a performance test. Meanwhile, the second experiment shows the
productivity benefits that a tester can gain by using the proposed approach.

Additionally two environment configurations were used, as shown in
\figurename ~\ref{fig_env}. One was composed of an RPT node, one application
node and a \emph{WAIT Server} node; the other was composed of a RPT node, a load
balancer node, two application nodes and a \emph{WAIT Server} node. All
connected by a 10-GBit LAN.

\begin{figure}[!h]
\centering
\includegraphics[totalheight=.15\textheight,width=0.9\textwidth]{Environments}
\caption{Environment Configurations}
\label{fig_env}
\end{figure}

The RPT node ran over Windows XP with an Intel Xeon CPU at
2.67 GHz and 3GB of RAM using RPT 8.2.1.3. The \emph{WAIT Server} was run over
Red Hat Enterprise Linux Server 5.9, with an Intel Xeon CPU at 2.66 GHz and 2GB of
RAM using Apache Web Server 2.2.3. Each application node was a 64-bit Windows
Server 2008, with an Intel Xeon E7-8860 CPU at 2.26 GHz and 4GB of RAM
running Java 1.6.0 IBM J9 VM (build 2.6). Finally, the load balancer node had
the same characteristics of the \emph{WAIT Server} node. 

\vspace{-7pt}
\subsection{Experiment \#1: Overhead Evaluation}
\vspace{-7pt}

The objective here was to quantify the overhead of the proposed approach and
involved the assessment of four metrics: Throughput (hits per second), response
time (milliseconds), CPU (\%) and memory (MB) utilization. All metrics were
collected through RPT. Furthermore, two real-world applications were used:
iBatis JPetStore 4.0 \footnote{http://sourceforge.net/projects/ibatisjpetstore/}
which is an upgraded version of Sun's Pet Store, an e-commerce shopping cart. It
ran over an Apache Tomcat 6.0.35. The other application was IBM WebSphere Portal 
8.0.1 \footnote{http://www-03.ibm.com/software/products/us/en/portalserver},
a leading solution in the enterprise portal market. It ran over an IBM WebSphere
Application Server 8.0.0.5.

Firstly, the overhead was measured in a single-node environment using three
combinations of WAIT: 
\vspace{-7pt}
\begin{itemize}
	\item The applications alone to get a baseline.
	\item The applications with manual WAIT data collection.
	\item The applications with an automated WAIT.
\end{itemize}

For each combination using WAIT, the \emph{Sampling Interval} was configured
to 480 seconds (a commonly used value) and 30 seconds (minimum value recommended
for WAIT). The remaining test configuration was suggested by IBM SVT to
reflect real-world conditions: A workload of 2,000 concurrent users; a duration
of 1 hour; a \emph{Hard Disk Threshold} of 100MB; and a \emph{Time Threshold} of
10 minutes. Finally, each combination was repeated three times.

For JPetStore, each test run produced around 500,000 transactions. The results
presented in \tablename ~\ref{PetStore1} showed that using WAIT with a
\emph{Sampling Interval} of 480 seconds had practically no impact in terms of
response time and throughput. Furthermore the difference in resource consumption
between the different modalities of WAIT was around 1\%.  This difference was
mostly related to the presence of the \emph{Node Agent} because the uploaded
data was very small in this case (around 200KB every 10 minutes). When a
\emph{Sampling Interval} of 30 seconds was used, the impact on response time
and throughput appeared. Since the throughput was similar between the WAIT
modalities, the impact was caused by the \emph{Javacore} generation as it is
the only step shared between the modalities. On average, the generation of a
\emph{Javacore} took around 1 second. Even though this cost was insignificant in
the higher \emph{Sampling Interval}, with 30 seconds the impact was visible. The
difference in response times (2.8\%, around 53 milliseconds) was caused by the
upload and backup processes (around 4MB of data every 10 minutes), as the cost
of the \emph{Node Agent} presence had been previously measured. In terms of resource 
consumption, the differences between the WAIT modalities remained within 1\%.

\vspace{-2pt}
\begin{table}[!h]
\caption{JPetStore - Overhead Results}
\label{PetStore1}
\centering
\begin{tabular}{p{0.2\textwidth}|p{0.16\textwidth}|p{0.16\textwidth}|p{0.15\textwidth}|p{0.13\textwidth}|p{0.16\textwidth}}
\hline
\bfseries WAIT Modality & \bfseries Avg Response Time (ms)& \bfseries Max
Response Time (ms)& \bfseries Avg Throughput (hps)& \bfseries Avg CPU Usage
(\%) & \bfseries Avg Memory Usage (MB)\\
\hline
None (\emph{Baseline}) 	& 1889.6	& 44704.0	& 158.8 	& 36.9 		& 1429\\
Manual, 480s 			& 0.0\% 	& 0.0\%		& 0.0\%		& 1.1\% 	& 3.0\%\\
Automated, 480s 		& 0.0\%		& 0.0\%		& 0.0\% 	& 2.0\% 	& 3.7\%\\
Manual, 30s 			& 1.6\%		& 0.4\%		& -3.1\% 	& 1.47\% 	& 4.1\%\\
Automated, 30s 			& 4.4\%		& 0.5\%		& -4.0\% 	& 2.53\% 	& 4.4\%\\
%\hline
%Automated, 480s (2 nodes)& 0.5\%	& 0.2\%		& -1.4\% 	& 0.85\% 	& 2.3\%\\
\hline
\end{tabular}
\end{table}
\vspace{-2pt}

For Portal, each test run produced around 400,000 transactions and the
results are presented in \tablename ~\ref{Portal1}. These show similar trends
to the results in \tablename ~\ref{PetStore1}, but a few
key differences were identified: First, the impact on response time and
throughput were visible even with the \emph{Sampling Interval} of 480 seconds.
Also, the differences between the results for the two \emph{Sampling Interval}
were bigger. As the experimental conditions were the same, it was initially
assumed that these differences were related to the dissimilar functionality of the 
tested applications. This was confirmed after analyzing the \emph{Javacores} generated 
by Portal, which allowed the differences in behavior of Portal to be quantified:
The average size of a \emph{Javacore} was 5.5MB (450\% bigger than JPetStore's), its 
average generation time was 2 sec (100\% bigger than JPetStore's), with a maximum 
generation time of 3 sec (100\% bigger than JPetStore's).

\vspace{-2pt}
\begin{table}[!h]
\caption{Portal - Overhead Results}
\label{Portal1}
\centering
\begin{tabular}{p{0.2\textwidth}|p{0.16\textwidth}|p{0.16\textwidth}|p{0.15\textwidth}|p{0.13\textwidth}|p{0.16\textwidth}}
\hline
\bfseries WAIT Modality & \bfseries Avg Response Time (ms)& \bfseries Max
Response Time (ms)& \bfseries Avg Throughput (hps)& \bfseries Avg CPU Usage
(\%) & \bfseries Avg Memory Usage (MB)\\
\hline
None (\emph{Baseline}) 	& 4704.75	& 40435.50	& 98.05 	& 76.73 	& 3171.20\\
Manual, 480s 			& 0.7\% 	& 0.6\%		& -0.1\%	& 0.63\% 	& 2.2\%\\
Automated, 480s 		& 3.4\%		& 1.0\%		& -2.8\% 	& 1.13\% 	& 4.1\%\\
Manual, 30s 			& 14.9\%	& 5.4\%		& -5.6\% 	& 2.23\% 	& 5.3\%\\
Automated, 30s 			& 16.8\%	& 9.1\%		& -5.7\% 	& 2.97\% 	& 6.0\%\\
\hline
\end{tabular}
\end{table}
\vspace{-2pt}

%<<
%!!!- Regenerate the Pair t-Test using the real data*** (avg/max rt->rt, avg
%thr->thr) + 0.05
%>>

To explore the small differences between the runs and the potential
environmental variations that were experienced during the experiments, a Paired
t-Test
\footnote{http://www.aspfree.com/c/a/braindump/comparing-data-sets-using-statistical-analysis-in-excel/}
was done (using a significance level of p\textless0.1) to evaluate if the
differences in response time and throughput were statistically significant. 
This analysis indicated that for JPetStore the only significant differences existed in the average response
time and the average throughput when using a
\emph{Sampling Interval} of 30 seconds. Similar results were obtained from Portal. This analysis reinforced the
conclusion that the overhead was low and the observation that the
\emph{Sampling Interval} of 480 seconds was preferable.

A second test was done to validate that the overhead remained low in a
multi-node environment over a longer test run. This test used JPetStore and the
automated WAIT tool with a \emph{Sampling Interval} of 480 seconds. The rest of
the set-up was identical to the previous tests except the workload which was doubled
to compensate for the additional application node and the test duration which
was increased to 24 hours. Even though the results were slightly different than the
single-node run, they proved that the solution was reliable, as using the
automated approach had minimal impact in terms of response time (0.5\% 
average and 0.2\% max) and throughput (1.4\%). Moreover the consumption
of resources behaved similarly to the single-node test (an increment of 0.85\%
in CPU and 2.3\% in Memory). A paired t-Test also indicated
that the differences in response time and throughput between the runs were
not significant.

In conclusion, the results of this experiment proved that the
overhead caused by the automated approach was low, therefore the results of a
performance test are not compromised. Due to the impact that the \emph{Sampling
Interval} and the application behavior could have on the overhead, it is important to 
consider these factors in the configuration. In our case, a \emph{Sampling
Interval} of 480 seconds proved efficient in terms of overhead for the two tested applications using WAIT.

\vspace{-7pt}
\subsection{Experiment \#2: Assessment of productivity benefits}
\label{Experiment_2_Results}
\vspace{-7pt}

Here the objective was to assess the benefits our approach brings to a
performance tester. First, the source code of JPetStore was modified and three
common performance issues were injected: 
\vspace{-7pt}
\begin{itemize}
	\item A lock contention bug, composed of a very heavy calculation within a
	synchronized block of code.
	\item An I/O latency bug, composed of a very expensive file reading method.
	\item A deadlock bug, composed of an implementation of the classic ``friends
	bowing'' deadlock
	example\footnote{http://docs.oracle.com/javase/tutorial/essential/concurrency/deadlock.html}.
\end{itemize}

Then an automated WAIT monitored the application to assess how well
it was able to identify the injected bugs and estimate the corresponding time
savings in performance analysis. All set-up parameters were identical to
the multi-node test previously described except the duration which
was one hour. Due to space constraints, only the most relevant sections
of the WAIT reports are presented.

Surprisingly the 1st ranked issue was none of the injected bugs but a method
named ``McastServiceImpl.receive'' which appeared in practically all the
samples. Further analysis determined this method call was benign and related
to the clustering functionality of Tomcat. The 2nd ranked issue was the lock
contention. It is worth noting that both issues were
detected in the early versions of the report from the tool and their high
frequency (above 96\% of the samples) could have led a tester to pass this
information to the development team so that the diagnosis could start far ahead
of the test completion. The final report reinforced the presence of these issues 
by offering similar rankings. \figurename ~\ref{fig_run1_bugs12}.a shows the
results of the early report, while ~\ref{fig_run1_bugs12}.b shows the results of 
the final report.

\begin{figure}[!h]
\centering
\includegraphics[totalheight=.25\textheight,width=1\textwidth]{big_issues12_run1_earlyVsFinalReport}
\caption{Top detected performance issues in modified JPetStore application}
\label{fig_run1_bugs12}
\end{figure}

\begin{figure}[!h]
\centering
\includegraphics[totalheight=.25\textheight,width=1\textwidth]{lockContentionIssue_WAITvsEclipse_Highlighted}
\caption{Lock contention issue in the WAIT report and the actual source code}
\label{fig_issue2_vs_code}
\end{figure}

After identifying an issue, a tester can see more details, including the type of
problem, involved class, method and method line. \figurename
~\ref{fig_issue2_vs_code} shows the information of our Lock Contention bug,
which was located in the class LockContentionBug, the method generateBug and the
line 20. When comparing this information with the actual code, one can see that
is precisely the line where the bug was injected (taking a class lock before
doing a very CPU intensive logic). In 3rd place the report showed a symptom of
the lock contention issue, suggesting this was a major problem (the issues were correlated 
by comparing their detailed information, which pinpointed to the same class and
method). Finally, the I/O latency bug was identified in 4th place. \figurename
~\ref{fig_issues34} shows the details of these issues.

\begin{figure}[!h]
\centering
\includegraphics[totalheight=.25\textheight,width=1\textwidth]{big_issue3and4_run1_finalReportDetailsMerge}
\caption{Details of issues ranked 3rd and 4th}
\label{fig_issues34}
\end{figure}

The deadlock issue did not appear in this test run, somehow prevented by the
lock contention bug which had a bigger impact that planned. As in any regular
test phase, the identified bugs were ``fixed'' and a new run was done to review
if any remaining performance issues existed. Not surprisingly, the deadlock bug
appeared. \figurename ~\ref{fig_dlissue_vs_code} shows the information of our
Deadlock bug, which was located in the line 30 of the DeadLockBug class. This is
precisely the line where the bug was injected (as the deadlock occurs when the
friends bow back to each other).
%\vspace{-5pt}
\begin{figure}[!h]
\centering
\includegraphics[totalheight=.23\textheight,width=1\textwidth]{deadlockIssue_DetailsVsEclipse_Highlight}
\caption{Deadlock issue in the WAIT report and the actual source code}
\label{fig_dlissue_vs_code}
\end{figure}
%\vspace{-5pt}

As all injected bugs were identified, including the involved classes and
methods, this experiment was considered successful. In terms of time, two main
savings were documented. First, the automated approach practically reduced the
effort of using WAIT to zero. After a one-time installation which took no more
than 15 minutes for all nodes, the only additional effort required to use the
automated approach were a few seconds spent configuring it (i.e. to change the
\emph{Sampling Interval}). The second time saving occurred in the analysis of the 
WAIT reports. Previously, a tester would have ended with multiple reports. Now a 
tester only needs to monitor a single report which is refreshed periodically.

Overcoming the usage constraints of WAIT also allowed to exploit
WAIT's expert knowledge capabilities. Even though it might be hard to define an
average time spent identifying performance issues, a conservative estimate of
2 hours per bug could help to quantify these savings. In our case study,
instead of spending an estimated 6 hours analyzing the issues, it was
possible to identify them and their root causes in a matter of minutes with the 
information provided by the WAIT report. As seen in the experiment, additional
time can also be saved if the relevant issues are reported to developers in parallel to
the test execution. This is especially valuable in long-term runs which
are common in performance testing and typically last several days.

To summarize these experimental results, they were very promising because it
was possible to measure the productivity benefits that a tester can gain by using
WAIT through our proposed automation approach: After a quick installation
(around 5 minutes per node), the time required to use the automated WAIT was
minimal. Moreover a tester now only needs to monitor a single WAIT report, which
offers a consolidated view of the results. A direct consequence of these
time savings is the reduction in the dependence on human expert knowledge and
reduced the effort required by a tester to identify performance issues, hence
improving the productivity.

\vspace{-7pt}
\subsection{Threats to Validity}
\vspace{-7pt}
Like any empirical work, there are some threats to the validity of these
experiments. First the possible environmental noise that could affect the test
environments because they are not isolated. To mitigate this, multiple runs were
executed for each identified combination. Another threat was the selection of
the tested applications. Despite being real-world applications, their limited
number implies that not all types of applications have been tested and wider
experiments are needed to get more general conclusions. However, there is no
reason to believe that the presented approach is not applicable to other
environments.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Related Work
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-7pt}
\section{Related Work}
\label{RelatedWork}
\vspace{-7pt}

The idea of applying automation in the performance testing domain is not new.
However, most of the research has focused on automating the generation of load
test
suites\cite{Elvira1,Bayan1,Zhang1,Briand1,Avritzer2,Avritzer3,Chen1,Garousi1}.
For example \cite{Bayan1} proposes an approach to automate the generation of test 
cases, hence simplifying this time-consuming task, based on specified levels of
load and combinations of resources. Similarly, \cite{Chen1} presents an
automation framework that separates the application logic from the performance
testing scripts.

Regarding performance analysis, a high percentage of the proposed
techniques require instrumentation. For example, the authors in \cite{Yang1}
instrument the source code to mine the sequences of call graphs to infer any
relevant error patterns. A similar case occurs with the works presented in
\cite{Hangal1,Csallner1} which rely on instrumentation to dynamically infer
invariants and detect programming errors; or the approach proposed by
\cite{Chen2} which uses instrumentation to capture execution paths to determine
the distributions of normal paths and look for any significant deviations to
detect errors. In all these cases, instrumentation would obscure the performance
of an application during performance testing hence discouraging their usage.
This is especially relevant in load and stress testings, where it is common to
use huge workloads, which might trigger non-lineal overheads in the
instrumentation. On the contrary, our proposed approach does not require any
instrumentation.

Moreover the authors of \cite{Jiang2009} present a non-intrusive approach which
automatically analyzes the execution logs of a load test to identify performance
problems. As this approach only relies on load testing results, it can not
determine root causes. A similar approach is presented in \cite{Malik1} which
aims to offer information about the causes behind the issues. However it can
only provide the subsystem responsible of the performance deviation. On the
contrary, our approach allows the applicability of the idle-time analysis in the
performance testing domain through automation, which allows the identification
of the classes and methods responsible for the performance issues. Moreover the
techniques presented in \cite{Jiang2009,Malik1} require information from
previous runs to baseline their analysis, information which might not
always be available. Finally, the authors of \cite{mon3} presents a framework
for monitoring software services. The main difference between this work and ours is 
that our proposed approach has been designed to address the specific needs of
performance testing, isolating a tester from the complexities of an expert system, 
while \cite{mon3} has been designed for operational support.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Section 7: Conclusions
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-7pt}
\section{Conclusions and Future Work}
\label{Conclusions}
\vspace{-7pt}

The identification of performance problems and their root causes in highly
distributed environments are complex and time-consuming tasks. Even though
researchers have been developing expert systems to simplify these tasks, various
limitations exist that prevent their effective usage in performance testing. To
address these limitations, this work proposed a novel approach to automate the usage of 
an expert system in a distributed test environment. A prototype was developed
around the WAIT expert tool and then its benefits and overhead were assessed.
The results showed that for a \emph{Sampling Interval} of 480 seconds, the automation
caused a negligible performance degradation of the JPetstore application. For the Portal,
it caused a 3.4\% increase in average response time and a 2.8\% reduction in
throughput. The results also showed the time savings gained by applying the
approach to an expert tool. In our case, the effort to utilize WAIT, which used
to depend on the number of nodes and the sampling frequency, was
reduced to seconds regardless of the configuration. This optimization also
simplified the identification of performance issues. In our case study, all
defects injected in JPetStore were detected in a matter of minutes using WAIT's
consolidated outputs. In contrast, a manual analysis might have taken hours.
Thus, the approach was shown to have a low overhead and to provide an easily
accessible summary of the system performance in a single report. It therefore
has the possibility of reducing the time required to analyze performance issues,
thereby reducing the effort and costs associated with performance testing.

Future work will concentrate on assessing the approach and its benefits through
broader case studies with our industrial partner IBM with a special interest in 
the trade-off between the \emph{Sampling Interval} and the nature of the
applications.  It will also be investigated how best to exploit the
functional information that can now be obtained from a test environment (i.e.
workload, response time, throughput) to improve the idle-time analysis.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Section 8: Acknowledgements
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-8pt}
\section*{Acknowledgments}
\vspace{-7pt}
We would like to thanks Amarendra Darisa and Patrick O'Sullivan, from IBM SVT,
as their experience in performance testing helped us through the scope
definition and validation. This work was supported, in part, by Science
Foundation Ireland grant 10/CE/I1855 to Lero - the Irish Software Engineering
Research Centre (www.lero.ie).

\vspace{-8pt}
% Between 12 - 24 \ldots 18 sounds like a good number!
\bibliographystyle{splncs}
\bibliography{dwait_manual,dwait}

%\section*{Appendix: Figures Printing Test}

\end{document}


\cite{Gartner2008}
